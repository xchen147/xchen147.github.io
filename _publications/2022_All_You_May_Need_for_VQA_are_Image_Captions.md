---
title: "All You May Need for VQA are Image Captions"
collection: publications
permalink: /publication/2022_All_You_May_Need_for_VQA_are_Image_Captions
excerpt: 'We propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation.'
date: 2022-05-04
author: '*Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, **Xi Chen**, Nan Ding, Radu Soricut*' 
venue: 'arXiv:2205.01883 (2022)'
paperurl: 'https://arxiv.org/abs/2205.01883'
blogurl: 'https://ai.googleblog.com/2022/07/rewriting-image-captions-for-visual.html'

---

Visual Question Answering (VQA) has benefited from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-the-art zero-shot accuracy by double digits and achieve a level of robustness that lacks in the same model trained on human-annotated VQA data.

[Access the paper here](https://arxiv.org/abs/2205.01883)
