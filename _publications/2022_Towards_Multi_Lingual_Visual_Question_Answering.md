---
title: "Towards Multi-Lingual Visual Question Answering"
collection: publications
permalink: /publication/2022_Towards_Multi_Lingual_Visual_Question_Answering
excerpt: 'We propose scalable solutions to multi-lingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers.'
date: 2022-09-12
author: '*Soravit Changpinyo, Linting Xue, Idan Szpektor, Ashish V. Thapliyal, Julien Amelot, **Xi Chen**, Radu Soricut*' 
venue: 'arXiv:2209.05401 (2022)'
paperurl: 'http://academicpages.github.io/files/paper3.pdf'
---

Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require considerable amount of resources. In this paper, we propose scalable solutions to multi-lingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multi-lingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create MAVERICS-XM3600 (MaXM), a test-only VQA benchmark in 7 diverse languages. Finally, we propose an approach to unified, extensible, open-ended, and end-to-end mVQA modeling and demonstrate strong performance in 13 languages.

[Access the paper here](https://arxiv.org/abs/2209.05401)
